import hydra
from omegaconf import DictConfig
import torch
from pathlib import Path
import os
import logging
from src.model_module import MidiGenModule
from src.tokenizer_module import get_tokenizer
from tqdm import tqdm

log = logging.getLogger(__name__)

@hydra.main(version_base=None, config_path="configs", config_name="config")
def main(cfg: DictConfig):
    # 1. ì´ˆê¸° ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if hasattr(cfg, "compile_model"): cfg.compile_model = False # ì¶”ë¡  ì‹œ ì»´íŒŒì¼ ë„ê¸°

    # 2. ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = get_tokenizer(cfg)
    ckpt_dir = Path("checkpoints") / cfg.project_name
    ckpts = sorted(ckpt_dir.glob("*.ckpt"), key=os.path.getmtime)
    if not ckpts: return
    
    log.info(f">> ëª¨ë¸ ë¡œë“œ ì¤‘... {ckpts[-1]}")
    model_module = MidiGenModule(cfg, vocab_size=tokenizer.vocab_size)
    
    # ê°€ì¤‘ì¹˜ ë¡œë“œ ë° FP16/BF16 ë³€í™˜ (VRAM ì ˆì•½)
    checkpoint = torch.load(str(ckpts[-1]), map_location=device, weights_only=False)
    state_dict = {k.replace("model._orig_mod.", "model."): v for k, v in checkpoint["state_dict"].items()}
    model_module.load_state_dict(state_dict, strict=False, assign=True)
    
    # 4070 Ti Superë¼ë©´ bfloat16 ì¶”ì²œ
    model_module.to(dtype=torch.bfloat16).to(device).eval()

    # ==========================================
    # ğŸš€ ë¬´í•œ ìƒì„± ë£¨í”„ ì„¤ì •
    # ==========================================
    
    TOTAL_CHUNKS = 5       # 2048í† í° x 5ë²ˆ = ì•½ 10,000í† í° (ì›í•˜ëŠ” ë§Œí¼ ëŠ˜ë¦¬ì„¸ìš”)
    CHUNK_LEN = 2048       # í•œ ë²ˆì— ìƒì„±í•  ê¸¸ì´ (VRAM ì•ˆì „í•˜ê²Œ 2048 ì¶”ì²œ)
    
    # 1) ì‹œì‘ í”„ë¡¬í”„íŠ¸ (ì‘ê³¡ê°€ ì„¤ì •)
    current_ids = [tokenizer.start_token_id]
    if 'Global_Header' in tokenizer.tokens_structure:
        current_ids.append(tokenizer.tokens_structure['Global_Header'])
    
    target_composer = cfg.target_composer
    if hasattr(tokenizer, 'composer_map') and target_composer in tokenizer.composer_map:
        cid = tokenizer.composer_vocab_start + tokenizer.composer_map[target_composer]
        current_ids.append(cid)
        log.info(f">> ì‘ê³¡ê°€: {target_composer}")
    
    if 'Narrative_Stream' in tokenizer.tokens_structure:
        current_ids.append(tokenizer.tokens_structure['Narrative_Stream'])

    # ì „ì²´ ê³¡ì„ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
    full_song_ids = list(current_ids) 
    
    log.info(f"=== ğŸ¹ ë¬´í•œ ì‘ê³¡ ì‹œì‘ (ì´ {TOTAL_CHUNKS} ì¡°ê° ìƒì„± ì˜ˆì •) ===")

    for i in range(TOTAL_CHUNKS):
        torch.cuda.empty_cache() # ë©”ëª¨ë¦¬ ì²­ì†Œ
        
        # ì…ë ¥ ì¤€ë¹„: ì „ì²´ ê³¡ì´ ë„ˆë¬´ ê¸¸ë©´, ìµœê·¼ 2048~4096ê°œë§Œ ì˜ë¼ì„œ íŒíŠ¸ë¡œ ì¤Œ
        # TitansëŠ” ê¸°ì–µë ¥ì´ ì¢‹ì•„ì„œ ì•ë¶€ë¶„ì„ ë‹¤ì‹œ ì½ìœ¼ë©´ ë¬¸ë§¥ì„ ë³µì›í•¨
        context_window = 4096 
        input_context = full_song_ids[-context_window:] 
        
        input_tensor = torch.tensor([input_context], device=device).long()
        
        log.info(f">> [Chunk {i+1}/{TOTAL_CHUNKS}] ìƒì„± ì¤‘... (ì…ë ¥ ê¸¸ì´: {len(input_context)})")
        
        with torch.no_grad():
            # generate í•¨ìˆ˜ëŠ” ì…ë ¥+ì¶œë ¥ì„ ëª¨ë‘ ë°˜í™˜í•¨
            output = model_module.model.generate(
                input_tensor, 
                max_length=len(input_context) + CHUNK_LEN, # ì…ë ¥ ê¸¸ì´ + ìƒˆë¡œ ë§Œë“¤ ê¸¸ì´
                temperature=1.0,
                top_k=40
            )
            
        # ìƒˆë¡œ ìƒê¸´ ë¶€ë¶„ë§Œ ì˜ë¼ë‚´ê¸°
        new_tokens = output[0, len(input_context):].tolist()
        
        # ê²°ê³¼ í•©ì¹˜ê¸°
        full_song_ids.extend(new_tokens)
        log.info(f"   -> {len(new_tokens)} í† í° ìƒì„±ë¨. (í˜„ì¬ ì´ ê¸¸ì´: {len(full_song_ids)})")

        # ì¤‘ê°„ ì €ì¥ (í˜¹ì‹œ êº¼ì§ˆê¹Œë´)
        if (i + 1) % 1 == 0:
            save_path = Path(f"generated_output/{cfg.project_name}/infinite_temp.mid")
            try:
                valid_tokens = [t for t in full_song_ids if t < tokenizer.vocab_size]
                tokenizer.decode(valid_tokens, save_path)
            except: pass

    # ==========================================
    # ğŸ’¾ ìµœì¢… ì €ì¥
    # ==========================================
    final_path = Path(f"generated_output/{cfg.project_name}/titans_{target_composer.replace(' ', '_')}_full_length.mid")
    valid_tokens = [t for t in full_song_ids if t < tokenizer.vocab_size]
    
    log.info(f">> ìµœì¢… ë³€í™˜ ì¤‘... (ì´ {len(valid_tokens)} í† í°)")
    try:
        tokenizer.decode(valid_tokens, final_path)
        log.info(f"=== ğŸ‰ ì™„ì„±! ì €ì¥ë¨: {final_path} ===")
    except Exception as e:
        log.error(f"ë””ì½”ë”© ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main()