import hydra
from omegaconf import DictConfig
import torch
from pathlib import Path
import os
import logging
from src.model_module import MidiGenModule
from src.tokenizer_module import get_tokenizer

log = logging.getLogger(__name__)

@hydra.main(version_base=None, config_path="configs", config_name="config")
def main(cfg: DictConfig):
    # 1. ì„¤ì • ë° ë””ë°”ì´ìŠ¤ ì¤€ë¹„
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model_type = cfg.model.type.upper() if hasattr(cfg, "model") else "UNKNOWN"
    log.info(f"=== {model_type} ì‘ê³¡ ì‹œì‘ (Device: {device}) ===")
    
    # [ì¤‘ìš”] ì¶”ë¡  ì‹œì—ëŠ” ì»´íŒŒì¼ ê¸°ëŠ¥ì„ ë•ë‹ˆë‹¤ (ì˜¤ë¥˜ ë°©ì§€)
    if hasattr(cfg, "compile_model"):
        cfg.compile_model = False

    # 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = get_tokenizer(cfg)
    log.info(f">> Tokenizer loaded. Vocab size: {tokenizer.vocab_size}")

    # 3. ëª¨ë¸ ìˆ˜ë™ ì´ˆê¸°í™” (load_from_checkpoint ëŒ€ì‹  ì‚¬ìš©)
    # ì´ìœ : assign=True ì˜µì…˜ì„ ì‚¬ìš©í•˜ê³ , ì»´íŒŒì¼ëœ ì ‘ë‘ì‚¬ë¥¼ ì œê±°í•˜ê¸° ìœ„í•¨
    ckpt_dir = Path(cfg.paths.checkpoints)
    ckpts = sorted(ckpt_dir.glob("*.ckpt"), key=os.path.getmtime)
    if not ckpts:
        log.error(f"!! ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤: {ckpt_dir}")
        return
    
    ckpt_path = str(ckpts[-1])
    log.info(f">> ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {ckpt_path}")

    # (1) ëª¨ë¸ ê»ë°ê¸° ìƒì„± (ì»´íŒŒì¼ ë˜ì§€ ì•Šì€ ìˆœì • ìƒíƒœ)
    model_module = MidiGenModule(cfg, vocab_size=tokenizer.vocab_size)
    
    # (2) ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë¡œë“œ
    checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False) # weights_only=False for safety with older pytorch
    state_dict = checkpoint["state_dict"]

    # (3) í‚¤ ì´ë¦„ ì •ë¦¬ (ì»´íŒŒì¼ëœ ëª¨ë¸ì˜ '_orig_mod' ì œê±°)
    new_state_dict = {}
    for k, v in state_dict.items():
        # 'model._orig_mod.model.x' -> 'model.model.x' (MidiGenModule êµ¬ì¡°ì— ë§ì¶¤)
        # ë˜ëŠ” 'model.model.x'ê°€ ê·¸ëŒ€ë¡œ ìˆì„ ìˆ˜ë„ ìˆìŒ
        new_key = k.replace("model._orig_mod.", "model.") 
        new_state_dict[new_key] = v

    # (4) ê°€ì¤‘ì¹˜ ë¡œë“œ (í•µì‹¬: assign=True)
    # assign=TrueëŠ” í…ì„œ ê°’ì„ ë³µì‚¬í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼ í¬ì¸í„°ë¥¼ êµì²´í•˜ë¯€ë¡œ
    # "shared memory location" ì˜¤ë¥˜ë¥¼ ì™„ë²½í•˜ê²Œ í•´ê²°í•©ë‹ˆë‹¤.
    try:
        model_module.load_state_dict(new_state_dict, strict=False, assign=True)
        log.info(">> ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ (assign=True ì ìš©)")
    except Exception as e:
        log.error(f"!! ëª¨ë¸ ë¡œë“œ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        return

    model_module.to(device)
    model_module.eval()
    
    PRIMING_MIDI_PATH = "data/raw/maestro-v3.0.0/2004/MIDI-Unprocessed_Schubert4-6_MID--AUDIO_10_R2_2018_wav.midi"
    
    start_tokens = [tokenizer.start_token_id]
    
    # (1) Global Header (Anticipation í˜¸í™˜ì„± ìœ ì§€)
    if hasattr(tokenizer, 'tokens_structure') and 'Global_Header' in tokenizer.tokens_structure:
        start_tokens.append(tokenizer.tokens_structure['Global_Header'])
    
    # (2) ì‘ê³¡ê°€ ì„ íƒ (REMIì—ì„œëŠ” ë¬´ì‹œë¨)
    target_composer = cfg.target_composer
    if hasattr(tokenizer, 'composer_map') and target_composer in tokenizer.composer_map:
        composer_id = tokenizer.composer_vocab_start + tokenizer.composer_map[target_composer]
        start_tokens.append(composer_id)
        log.info(f">> ì„ íƒëœ ì‘ê³¡ê°€: {target_composer} (ID: {composer_id})")
    
    # (3) Narrative Stream (Anticipation í˜¸í™˜ì„± ìœ ì§€)
    if hasattr(tokenizer, 'tokens_structure') and 'Narrative_Stream' in tokenizer.tokens_structure:
        start_tokens.append(tokenizer.tokens_structure['Narrative_Stream'])

    # (4) [í•µì‹¬] í”„ë¼ì´ë°(Priming): ë°˜ì£¼ ì§€ì˜¥ íƒˆì¶œì„ ìœ„í•œ ê°•ì œ ì£¼ì…
    priming_tokens = []
    if os.path.exists(PRIMING_MIDI_PATH):
        log.info(f">> í”„ë¼ì´ë°(Priming) ì‹œë„: {PRIMING_MIDI_PATH} ì°¸ê³  ì¤‘...")
        try:
            # MIDI íŒŒì¼ì„ í† í°ìœ¼ë¡œ ë³€í™˜ (REMI í† í¬ë‚˜ì´ì € ì‚¬ìš©)
            full_tokens = tokenizer.encode(PRIMING_MIDI_PATH)
            
            # ì•ì—ì„œë¶€í„° 200ê°œ ì •ë„ ìë¥´ê¸° (ë„ì…ë¶€~ì´ˆë°˜ ë©œë¡œë””)
            # ë„ˆë¬´ ê¸¸ë©´ ìƒì„±í•  ê³µê°„ì´ ì¤„ì–´ë“œë‹ˆ ì ë‹¹íˆ ìë¦…ë‹ˆë‹¤.
            priming_tokens = full_tokens[:200]
            
            # ì¤‘ë³µëœ ì‹œì‘ í† í° ì œê±°
            if priming_tokens and priming_tokens[0] == tokenizer.start_token_id:
                priming_tokens = priming_tokens[1:]
                
            log.info(f">> í”„ë¼ì´ë° í† í° {len(priming_tokens)}ê°œ ì£¼ì… ì™„ë£Œ! (ë°˜ì£¼ íŒ¨í„´ íƒˆì¶œ ìœ ë„)")
        except Exception as e:
            log.warning(f"!! í”„ë¼ì´ë° íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ (ê¸°ë³¸ ëª¨ë“œë¡œ ì‹œì‘í•©ë‹ˆë‹¤): {e}")
    else:
        # ê²½ë¡œê°€ í‹€ë ¸ê±°ë‚˜ íŒŒì¼ì´ ì—†ìœ¼ë©´ ê²½ê³  ë©”ì‹œì§€ë¥¼ ë„ìš°ê³  ê·¸ëƒ¥ ì§„í–‰í•©ë‹ˆë‹¤.
        log.warning(f"!! í”„ë¼ì´ë° MIDI íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {PRIMING_MIDI_PATH}")
        log.warning("   -> ê²½ë¡œë¥¼ í™•ì¸í•˜ê±°ë‚˜, ê·¸ëƒ¥ ê¹¡í†µ(BOS) ìƒíƒœë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")

    # ìµœì¢… ì…ë ¥: [Start] + [Metadata] + [Priming(ë©œë¡œë””)]
    final_input_tokens = start_tokens + priming_tokens
    input_ids = torch.tensor([final_input_tokens], device=device).long()

    # 5. ìƒì„± (Generation)
    # í”„ë¼ì´ë° ê¸¸ì´ë§Œí¼ ëª©í‘œ ê¸¸ì´ë¥¼ ëŠ˜ë ¤ì¤ë‹ˆë‹¤.
    total_target_len = cfg.target_length + len(priming_tokens)
    log.info(f">> ìƒì„± ì‹œì‘... (ì…ë ¥: {len(final_input_tokens)} -> ëª©í‘œ: {total_target_len})")
    
    with torch.no_grad():
        generated_ids = model_module.model.generate(
            input_ids, 
            max_length=total_target_len, 
            temperature=1.0,     # ì°½ì˜ì„± 1.0 ìœ ì§€
            top_k=80,            # 80 ìœ ì§€ (ë‹¤ì–‘ì„± í™•ë³´)
            top_p=0.95,          # 0.95 ìœ ì§€
            # repetition_penaltyëŠ” ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ìƒëµ
        )

    # 6. ì €ì¥
    final_sequence = generated_ids[0].tolist()
    log.info(f">> ìƒì„± ì™„ë£Œ! ì´ ê¸¸ì´: {len(final_sequence)}")

    final_midi_ids = [t for t in final_sequence if t < tokenizer.vocab_size]
    
    output_dir = Path(cfg.paths.outputs)
    os.makedirs(output_dir, exist_ok=True)
    
    save_filename = f"titans_{target_composer.replace(' ', '_')}_len{len(final_midi_ids)}.mid"
    save_path = output_dir / save_filename
    
    log.info(">> MIDI ë³€í™˜ ì¤‘...")
    try:
        tokenizer.decode(final_midi_ids, save_path)
        log.info(f"=== ğŸ¹ ì‘ê³¡ ì™„ë£Œ! ì €ì¥ë¨: {save_path} ===")
    except Exception as e:
        log.error(f"!! ë””ì½”ë”© ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main()