import hydra
from omegaconf import DictConfig
import torch
from pathlib import Path
import os
import logging
from src.model_module import MidiGenModule
from src.tokenizer_module import get_tokenizer

log = logging.getLogger(__name__)

@hydra.main(version_base=None, config_path="configs", config_name="config")
def main(cfg: DictConfig):
    # 1. ì„¤ì • ë° ë””ë°”ì´ìŠ¤ ì¤€ë¹„
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    log.info(f"=== Titans(MAC) ì‘ê³¡ ì‹œì‘ (Device: {device}) ===")
    
    # [ì¤‘ìš”] ì¶”ë¡  ì‹œì—ëŠ” ì»´íŒŒì¼ ê¸°ëŠ¥ì„ ë•ë‹ˆë‹¤ (ì˜¤ë¥˜ ë°©ì§€)
    if hasattr(cfg, "compile_model"):
        cfg.compile_model = False

    # 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = get_tokenizer(cfg)
    log.info(f">> Tokenizer loaded. Vocab size: {tokenizer.vocab_size}")

    # 3. ëª¨ë¸ ìˆ˜ë™ ì´ˆê¸°í™” (load_from_checkpoint ëŒ€ì‹  ì‚¬ìš©)
    # ì´ìœ : assign=True ì˜µì…˜ì„ ì‚¬ìš©í•˜ê³ , ì»´íŒŒì¼ëœ ì ‘ë‘ì‚¬ë¥¼ ì œê±°í•˜ê¸° ìœ„í•¨
    ckpt_dir = Path("checkpoints") / cfg.project_name
    ckpts = sorted(ckpt_dir.glob("*.ckpt"), key=os.path.getmtime)
    if not ckpts:
        log.error(f"!! ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤: {ckpt_dir}")
        return
    
    ckpt_path = str(ckpts[-1])
    log.info(f">> ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {ckpt_path}")

    # (1) ëª¨ë¸ ê»ë°ê¸° ìƒì„± (ì»´íŒŒì¼ ë˜ì§€ ì•Šì€ ìˆœì • ìƒíƒœ)
    model_module = MidiGenModule(cfg, vocab_size=tokenizer.vocab_size)
    
    # (2) ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë¡œë“œ
    checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False) # weights_only=False for safety with older pytorch
    state_dict = checkpoint["state_dict"]

    # (3) í‚¤ ì´ë¦„ ì •ë¦¬ (ì»´íŒŒì¼ëœ ëª¨ë¸ì˜ '_orig_mod' ì œê±°)
    new_state_dict = {}
    for k, v in state_dict.items():
        # 'model._orig_mod.model.x' -> 'model.model.x' (MidiGenModule êµ¬ì¡°ì— ë§ì¶¤)
        # ë˜ëŠ” 'model.model.x'ê°€ ê·¸ëŒ€ë¡œ ìˆì„ ìˆ˜ë„ ìˆìŒ
        new_key = k.replace("model._orig_mod.", "model.") 
        new_state_dict[new_key] = v

    # (4) ê°€ì¤‘ì¹˜ ë¡œë“œ (í•µì‹¬: assign=True)
    # assign=TrueëŠ” í…ì„œ ê°’ì„ ë³µì‚¬í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼ í¬ì¸í„°ë¥¼ êµì²´í•˜ë¯€ë¡œ
    # "shared memory location" ì˜¤ë¥˜ë¥¼ ì™„ë²½í•˜ê²Œ í•´ê²°í•©ë‹ˆë‹¤.
    try:
        model_module.load_state_dict(new_state_dict, strict=False, assign=True)
        log.info(">> ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ (assign=True ì ìš©)")
    except Exception as e:
        log.error(f"!! ëª¨ë¸ ë¡œë“œ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        return

    model_module.to(device)
    model_module.eval()
    
    # 4. í”„ë¡¬í”„íŠ¸ ìƒì„± (Conditioning)
    start_tokens = [tokenizer.start_token_id]
    
    # Global Header
    if 'Global_Header' in tokenizer.tokens_structure:
        start_tokens.append(tokenizer.tokens_structure['Global_Header'])
    
    # ì‘ê³¡ê°€ ì„ íƒ
    target_composer = cfg.target_composer
    if hasattr(tokenizer, 'composer_map') and target_composer in tokenizer.composer_map:
        composer_id = tokenizer.composer_vocab_start + tokenizer.composer_map[target_composer]
        start_tokens.append(composer_id)
        log.info(f">> ì„ íƒëœ ì‘ê³¡ê°€: {target_composer} (ID: {composer_id})")
    else:
        log.warning(f"!! ì‘ê³¡ê°€ '{target_composer}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # Narrative Stream
    if 'Narrative_Stream' in tokenizer.tokens_structure:
        start_tokens.append(tokenizer.tokens_structure['Narrative_Stream'])

    input_ids = torch.tensor([start_tokens], device=device).long()

    # 5. ìƒì„± (Generation)
    log.info(f">> ìƒì„± ì‹œì‘... ëª©í‘œ ê¸¸ì´: {cfg.target_length} í† í°")
    
    with torch.no_grad():
        generated_ids = model_module.model.generate(
            input_ids, 
            max_length=cfg.target_length,
            temperature=1.0,
            top_k=20,
            repetition_penalty=1.1,
        )

    # 6. ì €ì¥
    final_sequence = generated_ids[0].tolist()
    log.info(f">> ìƒì„± ì™„ë£Œ! ì´ ê¸¸ì´: {len(final_sequence)}")

    final_midi_ids = [t for t in final_sequence if t < tokenizer.vocab_size]
    
    output_dir = Path("generated_output") / cfg.project_name
    os.makedirs(output_dir, exist_ok=True)
    
    save_filename = f"titans_{target_composer.replace(' ', '_')}_len{len(final_midi_ids)}.mid"
    save_path = output_dir / save_filename
    
    log.info(">> MIDI ë³€í™˜ ì¤‘...")
    try:
        tokenizer.decode(final_midi_ids, save_path)
        log.info(f"=== ğŸ¹ ì‘ê³¡ ì™„ë£Œ! ì €ì¥ë¨: {save_path} ===")
    except Exception as e:
        log.error(f"!! ë””ì½”ë”© ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main()