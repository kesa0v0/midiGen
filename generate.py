import hydra
from omegaconf import DictConfig
import torch
from miditok import REMI, TokenizerConfig
from pathlib import Path
import os
import json
from src.model_module import MidiGenModule # Lightning Module ë¶ˆëŸ¬ì˜¤ê¸°
from tqdm import tqdm

@hydra.main(version_base=None, config_path="configs", config_name="config")
def main(cfg: DictConfig):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"=== Midigen Lightning ì‘ê³¡ ì‹œì‘ (Device: {device}) ===")

    # 1. ì‘ê³¡ê°€ ë§¤í•‘ ì •ë³´ ë¡œë“œ
    if not os.path.exists("composer_map.json"):
        print("!! composer_map.jsonì´ ì—†ìŠµë‹ˆë‹¤. ëœë¤ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        composer_token_id = 0
        target_composer = "Unknown"
    else:
        with open("composer_map.json", "r") as f:
            mapping_info = json.load(f)
        composer_to_id = mapping_info["composer_to_id"]
        base_vocab_size = mapping_info["base_vocab_size"]
        
        target_composer = "FrÃ©dÃ©ric Chopin" 
        if target_composer not in composer_to_id:
            target_composer = list(composer_to_id.keys())[0]
        
        composer_token_id = base_vocab_size + composer_to_id[target_composer]
        print(f">> ì„ íƒëœ ì‘ê³¡ê°€: {target_composer} (ID: {composer_token_id})")

    # 2. í† í¬ë‚˜ì´ì € ë¡œë“œ (í‘œì¤€ REMI)
    beat_res_dict = eval(cfg.data.beat_res) if isinstance(cfg.data.beat_res, str) else cfg.data.beat_res
    tokenizer_config = TokenizerConfig(
        num_velocities=cfg.data.num_velocities, 
        use_chords=True,
        use_tempos=True,
        beat_res=beat_res_dict
    )
    tokenizer = REMI(tokenizer_config)

    # 3. ëª¨ë¸ ë¡œë“œ
    # checkpoints í´ë”ì—ì„œ ê°€ì¥ ìµœì‹  .ckpt íŒŒì¼ì„ ì°¾ìŒ
    ckpts = sorted(Path("checkpoints").glob("*.ckpt"), key=os.path.getmtime)
    if not ckpts:
        print("!! ì²´í¬í¬ì¸íŠ¸(.ckpt)ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € train.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    ckpt_path = str(ckpts[-1])
    print(f">> ë¡œë“œ ì¤‘: {ckpt_path}")

    # ëª¨ë¸ êµ¬ì¡° + ê°€ì¤‘ì¹˜ ìë™ ë³µêµ¬
    # model_module = MidiGenModule.load_from_checkpoint(ckpt_path, cfg=cfg)
    cfg.compile_model = False  # ìƒì„± ì‹œì—ëŠ” ì»´íŒŒì¼ ë¹„í™œì„±í™”
    model_module = MidiGenModule(cfg)

    checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)
    state_dict = checkpoint["state_dict"]

    # í‚¤ ì´ë¦„ ë³€ê²½ ('model._orig_mod.' -> 'model.')
    new_state_dict = {}
    for k, v in state_dict.items():
        new_key = k.replace("model._orig_mod.", "model.") # ì»´íŒŒì¼ëœ ì ‘ë‘ì‚¬ ì œê±°
        new_state_dict[new_key] = v
        
    # ìˆ˜ì •ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
    model_module.load_state_dict(new_state_dict)
    
    # ëª¨ë¸ì„ bfloat16ìœ¼ë¡œ ë³€í™˜ (Flash Attention 2 í•„ìˆ˜ ì¡°ê±´!)
    model_module.model.to(dtype=torch.bfloat16) 
    model_module.to(device)
    model_module.eval()
    
    # ì‹¤ì œ GPT-2 ëª¨ë¸ êº¼ë‚´ê¸°
    model = model_module.model 


    # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„± ì„¤ì •
    TARGET_LENGTH = cfg.target_length if cfg.target_length or cfg.target_length <= 1024 else 1024  # ëª©í‘œ ê³¡ ê¸¸ì´ (í† í° ìˆ˜). ì•½ 3~4ë¶„ ë¶„ëŸ‰
    CONTEXT_WINDOW = cfg.data.max_seq_len  # ëª¨ë¸ì´ í•œ ë²ˆì— ë³¼ ìˆ˜ ìˆëŠ” ìµœëŒ€ ê¸¸ì´ (í•™ìŠµ ì„¤ì •ê³¼ ë™ì¼í•´ì•¼ í•¨)
    NEW_TOKENS_PER_STEP = cfg.data.max_seq_len // 2  # í•œ ë²ˆì— ìƒì„±í•  ê¸¸ì´ (CONTEXT_WINDOWì˜ ì ˆë°˜ ì¶”ì²œ)

    # 4. ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ ê¸´ ê³¡ ìƒì„±

    current_ids = [composer_token_id, tokenizer["Bar_None"]]
    full_generated_sequence = list(current_ids)
    
    TARGET_LENGTH = cfg.target_length if hasattr(cfg, "target_length") and cfg.target_length and cfg.target_length <= 1024 * 16 else 1024 * 4  # ê¸°ë³¸ 4096
    CONTEXT_WINDOW = cfg.data.max_seq_len
    NEW_TOKENS_PER_STEP = CONTEXT_WINDOW // 2

    print(f">> ëª©í‘œ ê¸¸ì´: {TARGET_LENGTH} í† í° (ìŠ¬ë¼ì´ë”© ë°©ì‹)")
    pbar = tqdm(total=TARGET_LENGTH, desc="ì‘ê³¡ ì¤‘")
    pbar.update(len(full_generated_sequence))

    while len(full_generated_sequence) < TARGET_LENGTH:
        # 1) ì…ë ¥ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (ê°€ì¥ ìµœê·¼ í† í°ë“¤ë§Œ ì˜ë¼ì„œ ê°€ì ¸ì˜´)
        max_context = CONTEXT_WINDOW - NEW_TOKENS_PER_STEP
        input_ids = full_generated_sequence[-max_context:]

        # 2) ìƒì„±
        with torch.no_grad():
            gen_len = len(input_ids) + NEW_TOKENS_PER_STEP
            output = model.generate(
                input_ids=torch.tensor([input_ids]).to(device),
                max_length=gen_len,
                do_sample=True,
                temperature=1.0,
                top_p=0.9,
                top_k=40,
                repetition_penalty=1.15,
                no_repeat_ngram_size=32,
                pad_token_id=0
            )

        # 3) ìƒˆë¡œìš´ í† í°ë§Œ ì¶”ì¶œí•´ì„œ ì „ì²´ ì‹œí€€ìŠ¤ì— ì¶”ê°€
        new_tokens = output[0, len(input_ids):].tolist()
        full_generated_sequence.extend(new_tokens)
        pbar.update(len(new_tokens))

    pbar.close()

    # 5. ì €ì¥
    print("\n>> ë³€í™˜ ë° ì €ì¥ ì¤‘...")
    final_midi_ids = [t for t in full_generated_sequence if t < len(tokenizer)]
    generated_midi = tokenizer.decode([final_midi_ids])
    save_path = f"output_{target_composer.replace(' ', '_')}_long.mid"
    generated_midi.dump_midi(save_path)
    print(f"\n=== ğŸ¹ ê¸´ ê³¡ ì‘ê³¡ ì™„ë£Œ! ì €ì¥ë¨: {save_path} ===")

if __name__ == "__main__":
    main()