import hydra
from omegaconf import DictConfig
import torch
from miditok import REMI, TokenizerConfig
from pathlib import Path
import os
import json
from src.models import MidigenTitans
from collections import Counter

@hydra.main(version_base=None, config_path="configs", config_name="config")
def main(cfg: DictConfig):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"=== Midigen V2 [Composer Control] ì‘ê³¡ ì‹œì‘ (Device: {device}) ===")

    # 1. ë§¤í•‘ ì •ë³´ ë¡œë“œ (Composer ID ì•Œê¸° ìœ„í•´)
    if not os.path.exists("composer_map.json"):
        raise FileNotFoundError("composer_map.jsonì´ ì—†ìŠµë‹ˆë‹¤. preprocess.pyë¥¼ ë¨¼ì € ì‹¤í–‰í–ˆë‚˜ìš”?")
    
    with open("composer_map.json", "r") as f:
        mapping_info = json.load(f)
    
    composer_to_id = mapping_info["composer_to_id"]
    base_vocab_size = mapping_info["base_vocab_size"]
    
    # [ì„¤ì •] ì›í•˜ëŠ” ì‘ê³¡ê°€ ì´ë¦„ì„ ì—¬ê¸°ì— ì ìœ¼ì„¸ìš”! (JSON íŒŒì¼ ì°¸ê³ )
    # ì˜ˆ: "FrÃ©dÃ©ric Chopin", "Ludwig van Beethoven", "Johann Sebastian Bach"
    target_composer = "FrÃ©dÃ©ric Chopin" 
    
    if target_composer not in composer_to_id:
        print(f"!! ê²½ê³ : '{target_composer}'ëŠ” ëª©ë¡ì— ì—†ìŠµë‹ˆë‹¤. ëœë¤ìœ¼ë¡œ ì•„ë¬´ë‚˜ ê³ ë¦…ë‹ˆë‹¤.")
        target_composer = list(composer_to_id.keys())[0]

    composer_token_id = base_vocab_size + composer_to_id[target_composer]
    print(f">> ì„ íƒëœ ì‘ê³¡ê°€: {target_composer} (Token ID: {composer_token_id})")

    # 2. í† í¬ë‚˜ì´ì € ë¡œë“œ (Resolution ì„¤ì • ì ìš©)
    # config.yamlì˜ beat_res ë¬¸ìì—´ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    beat_res_dict = eval(cfg.data.beat_res)
    
    tokenizer_config = TokenizerConfig(
        num_velocities=cfg.data.num_velocities, 
        use_chords=cfg.data.use_chords,
        beat_res=beat_res_dict # [ì¤‘ìš”] V2ì—ì„œ ë°”ë€ í•´ìƒë„ ì ìš©
    )
    tokenizer = REMI(tokenizer_config)

    # 3. ëª¨ë¸ ë¡œë“œ
    model = MidigenTitans(cfg).to(device)

    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (Loss 0.7ì§œë¦¬ ê´´ë¬¼ ë¡œë“œ)
    ckpts = sorted(Path("checkpoints").glob("*.pt"), key=os.path.getmtime)
    if not ckpts:
        print("!! ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ")
        return
    ckpt_path = str(ckpts[-1])
    print(f">> ë¡œë“œ ì¤‘: {ckpt_path}")
    
    model.load_state_dict(torch.load(ckpt_path, map_location=device, weights_only=True))
    model.eval()

    # 4. ì‹œë“œ(Seed) ì¤€ë¹„: [ì‘ê³¡ê°€ í† í°] + [ë„ë¯¸ì†”ë„ ë©œë¡œë””]
    print(">> ì‹œë“œ ë©œë¡œë”” ìƒì„± ì¤‘...")
    
    # (1) ì‘ê³¡ê°€ í† í°
    seed_ids = [composer_token_id] 
    
    # (2) ë©œë¡œë”” (ë„-ë¯¸-ì†”-ë„)
    def get_token(prefix, default_idx=0):
        # prefixë¡œ ì‹œì‘í•˜ëŠ” í† í° ì¤‘ í•˜ë‚˜ ì°¾ê¸°
        candidates = [t for t in tokenizer.vocab if t.startswith(prefix)]
        if candidates:
            # ì ë‹¹íˆ ì¤‘ê°„ê°’ í˜¹ì€ ì •ë ¬ í›„ ì„ íƒ
            candidates.sort(key=lambda x: int(x.split('_')[1]) if '_' in x and x.split('_')[1].isdigit() else x)
            return tokenizer[candidates[len(candidates)//2]]
        return tokenizer["Bar_None"] # Fallback

    try:
        # V2ëŠ” í•´ìƒë„ê°€ ë‹¬ë¼ì„œ í† í° ì´ë¦„ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë™ì  ê²€ìƒ‰
        # Position, Duration ë“±ì€ í† í¬ë‚˜ì´ì € vocabì—ì„œ ê²€ìƒ‰í•´ì„œ êµ¬ì„±
        
        # ê°„ëµí™”ëœ ì‹œë“œ ì£¼ì… (ì˜¤ë¥˜ ë°©ì§€ ìœ„í•´ ë‹¨ìˆœí™”)
        # ì‘ê³¡ê°€ í† í°ë§Œ ì¤˜ë„ ìŠ¤íƒ€ì¼ì´ ë‚˜ì˜µë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì‘ê³¡ê°€ í† í° + ì‹œì‘(Bar)ë§Œ ì¤ë‹ˆë‹¤.
        # ì‹œë“œ ë©œë¡œë””ê¹Œì§€ ë„£ìœ¼ë©´ ì¢‹ì§€ë§Œ, Resolution ë³€ê²½ìœ¼ë¡œ í† í° ì´ë¦„ ë§ì¶”ê¸° ê¹Œë‹¤ë¡œìš¸ ìˆ˜ ìˆìŒ.
        
        # ê·¸ë˜ë„ ë„ë¯¸ì†”ë„ëŠ” ë„£ì–´ë´…ì‹œë‹¤ (ë™ì  ê²€ìƒ‰)
        pitch_60 = tokenizer["Pitch_60"] if "Pitch_60" in tokenizer.vocab else tokenizer["NoteOn_60"]
        
        # ì‹œë“œì— ì¶”ê°€
        seed_ids.append(tokenizer["Bar_None"])
        seed_ids.append(pitch_60) # ì²« ìŒ 'ë„' í•˜ë‚˜ë§Œ ì¤˜ì„œ ì‹œì‘ ìœ ë„
        
        print(f">> ì‹œë“œ êµ¬ì„± ì™„ë£Œ: [Composer: {target_composer}] + [Bar] + [NoteOn_60]")

    except Exception as e:
        print(f"!! ì‹œë“œ êµ¬ì„± ì¤‘ ì—ëŸ¬ (ê¸°ë³¸ìœ¼ë¡œ ì§„í–‰): {e}")

    # 5. ìƒì„±
    print(">> ì‘ê³¡ ì¤‘...")

    print(f"DEBUG: ì‹œë“œ í† í°: {seed_ids}")
    print(f"DEBUG: ê°€ì¥ í° í† í° ID: {max(seed_ids)}")
    print(f"DEBUG: ëª¨ë¸ì´ ì•„ëŠ” ë‹¨ì–´ì¥ í¬ê¸°(Embedding Size): {model.token_emb.num_embeddings}")
    
    if max(seed_ids) >= model.token_emb.num_embeddings:
        print("!! [ì¹˜ëª…ì  ì˜¤ë¥˜] ì…ë ¥ í† í°ì´ ëª¨ë¸ì˜ ë‹¨ì–´ì¥ë³´ë‹¤ í½ë‹ˆë‹¤! Configì˜ vocab_sizeë¥¼ ëŠ˜ë ¤ì•¼ í•©ë‹ˆë‹¤.")
        exit()
    
    with torch.no_grad():
        generated_ids = model.generate(
            input_ids=torch.tensor([seed_ids]).to(device),
            max_length=1024,       # ê¸¸ê²Œ ë½‘ì•„ë´…ì‹œë‹¤
            temperature=0.95,       # ìì‹ ê°ì´ ìˆìœ¼ë‹ˆ 0.8 ì •ë„
            top_k=50,
            repetition_penalty=1.0 # íŒ¨ë„í‹° ê±°ì˜ ë” (ìì—°ìŠ¤ëŸ¬ìš´ ë°˜ë³µ í—ˆìš©)
        )

    # 6. ì €ì¥
    gen_token_ids = generated_ids[0].cpu().numpy().tolist()

    # í† í° í†µê³„ ì¶œë ¥
    # í† í° IDë¥¼ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    decoded_tokens = [tokenizer[tid] for tid in gen_token_ids if tid < len(tokenizer)]
    counts = Counter([t.split('_')[0] for t in decoded_tokens]) # Prefixë§Œ ì…ˆ (Pitch, Position...)
    
    print("\n=== í† í° í†µê³„ (ì´ ë¹„ìœ¨ì´ ì¤‘ìš”í•¨) ===")
    print(f"ì´ í† í° ìˆ˜: {len(decoded_tokens)}")
    print(f"ğŸµ ìŒí‘œ(Pitch/NoteOn): {counts.get('Pitch', 0) + counts.get('NoteOn', 0)}ê°œ")
    print(f"â³ ì‹œê°„ì´ë™(Position): {counts.get('Position', 0)}ê°œ")
    print(f"ğŸ“ ì§€ì†ì‹œê°„(Duration): {counts.get('Duration', 0)}ê°œ")
    
    if (counts.get('Pitch', 0) + counts.get('NoteOn', 0)) < 100:
        print("!! ê²½ê³ : ìŒí‘œê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤! ì—¬ì „íˆ ì‰¼í‘œë§Œ ì°ê³  ìˆìŠµë‹ˆë‹¤.")
    else:
        print(">> ìƒíƒœ ì–‘í˜¸: ìŒí‘œê°€ ì¶©ë¶„íˆ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ì‘ê³¡ê°€ í† í°(300ë²ˆëŒ€)ì€ MIDI ë³€í™˜ ì‹œ ì—ëŸ¬ë‚˜ë¯€ë¡œ ì œê±°í•´ì•¼ í•¨!
    # ê¸°ë³¸ vocab sizeë³´ë‹¤ í° IDëŠ” í•„í„°ë§
    valid_ids = [t for t in gen_token_ids if t < mapping_info["base_vocab_size"]]
    
    try:
        generated_midi = tokenizer.decode([valid_ids])
        save_path = f"output_{target_composer.replace(' ', '_')}.mid"
        generated_midi.dump_midi(save_path)
        print(f"\n=== ì‘ê³¡ ì™„ë£Œ! ì €ì¥ë¨: {save_path} ===")
        
    except Exception as e:
        print(f"ë³€í™˜ ì—ëŸ¬: {e}")
        # ë””ë²„ê¹…ìš©: Composer í† í°ì´ ì„ì—¬ ë“¤ì–´ê°”ëŠ”ì§€ í™•ì¸
        print(f"Max Token ID: {max(gen_token_ids)}")

if __name__ == "__main__":
    main()