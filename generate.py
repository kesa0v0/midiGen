import hydra
from omegaconf import DictConfig
import torch
from pathlib import Path
import os
import json
from src.model_module import MidiGenModule # Lightning Module ë¶ˆëŸ¬ì˜¤ê¸°
from tqdm import tqdm
from src.tokenizer_module import get_tokenizer # Import the new tokenizer factory
import logging

log = logging.getLogger(__name__)

@hydra.main(version_base=None, config_path="configs", config_name="config")
def main(cfg: DictConfig):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    log.info(f"=== Midigen Lightning ì‘ê³¡ ì‹œì‘ (Device: {device}) ===")

    # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ (ìƒˆë¡œìš´ ì¶”ìƒí™” ì‚¬ìš©)
    tokenizer = get_tokenizer(cfg)

    # 2. ì‘ê³¡ê°€ ë§¤í•‘ ì •ë³´ ë¡œë“œ
    composer_token_id = None
    target_composer = "Unknown"

    if os.path.exists("composer_map.json"):
        with open("composer_map.json", "r") as f:
            mapping_info = json.load(f)
        composer_to_id = mapping_info["composer_to_id"]
        
        # We need to ensure that the `base_vocab_size` from composer_map.json
        # is compatible with the current tokenizer's actual vocab_size.
        # If the new tokenizer has a different vocab size, the composer IDs might be incorrect.
        # For now, we'll assume the composer tokens are added after the base vocabulary.
        # This might need refinement if 'anticipation' handles composer info differently.
        base_vocab_size_from_map = mapping_info["base_vocab_size"]
        
        # Check if the current tokenizer's vocab size is at least the base_vocab_size from the map.
        if tokenizer.vocab_size < base_vocab_size_from_map:
            log.warning(
                "Tokenizer's current vocab size is smaller than base_vocab_size from composer_map.json. "
                "Composer tokens might be invalid. Proceeding with caution."
            )

        # configì—ì„œ target_composer ì½ê¸°
        target_composer = cfg.target_composer if hasattr(cfg, "target_composer") else "FrÃ©dÃ©ric Chopin"
        if target_composer not in composer_to_id:
            log.warning(f"Target composer '{target_composer}' not found in composer_map.json. Using first available composer.")
            target_composer = list(composer_to_id.keys())[0]
        
        # Assuming composer tokens are appended after the base vocabulary
        composer_token_id = tokenizer.vocab_size + composer_to_id[target_composer]
        log.info(f">> ì„ íƒëœ ì‘ê³¡ê°€: {target_composer} (ID: {composer_token_id})")
    else:
        log.warning("!! composer_map.jsonì´ ì—†ìŠµë‹ˆë‹¤. ì‘ê³¡ê°€ ì •ë³´ ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤. ì‹œì‘ í† í°ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        # If no composer map, use the start token as the initial token for generation
        composer_token_id = tokenizer.start_token_id


    # 3. ëª¨ë¸ ë¡œë“œ
    # checkpoints/{project_name} í´ë”ì—ì„œ ê°€ì¥ ìµœì‹  .ckpt íŒŒì¼ì„ ì°¾ìŒ
    ckpt_dir = Path("checkpoints") / cfg.project_name
    if not ckpt_dir.exists():
        log.error(f"!! ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {ckpt_dir}")
        return

    ckpts = sorted(ckpt_dir.glob("*.ckpt"), key=os.path.getmtime)
    if not ckpts:
        log.error(f"!! '{ckpt_dir}'ì— ì²´í¬í¬ì¸íŠ¸(.ckpt)ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € train.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    ckpt_path = str(ckpts[-1])
    log.info(f">> ë¡œë“œ ì¤‘: {ckpt_dir} -> {ckpt_path}")

    # ëª¨ë¸ êµ¬ì¡° + ê°€ì¤‘ì¹˜ ìë™ ë³µêµ¬
    cfg.compile_model = False  # ìƒì„± ì‹œì—ëŠ” ì»´íŒŒì¼ ë¹„í™œì„±í™”
    # Load correct vocab size from tokenizer
    vocab_size = tokenizer.vocab_size
    model_module = MidiGenModule(cfg, vocab_size=vocab_size)

    checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)
    state_dict = checkpoint["state_dict"]

    # í‚¤ ì´ë¦„ ë³€ê²½ ('model._orig_mod.' -> 'model.')
    new_state_dict = {}
    for k, v in state_dict.items():
        new_key = k.replace("model._orig_mod.", "model.") # ì»´íŒŒì¼ëœ ì ‘ë‘ì‚¬ ì œê±°
        new_state_dict[new_key] = v
        
    # ìˆ˜ì •ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
    model_module.load_state_dict(new_state_dict)
    
    # ëª¨ë¸ì„ bfloat16ìœ¼ë¡œ ë³€í™˜ (Flash Attention 2 í•„ìˆ˜ ì¡°ê±´!)
    model_module.model.to(dtype=torch.bfloat16)
    model_module.to(device)
    model_module.eval()
    
    # ì‹¤ì œ GPT-2 ëª¨ë¸ êº¼ë‚´ê¸°
    model = model_module.model 


    # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„± ì„¤ì •
    TARGET_LENGTH = cfg.target_length if hasattr(cfg, "target_length") and cfg.target_length and cfg.target_length <= cfg.tokenizer.max_seq_len * 16 else cfg.tokenizer.max_seq_len * 4  # ê¸°ë³¸ 4096
    CONTEXT_WINDOW = cfg.tokenizer.max_seq_len  # ëª¨ë¸ì´ í•œ ë²ˆì— ë³¼ ìˆ˜ ìˆëŠ” ìµœëŒ€ ê¸¸ì´ (í•™ìŠµ ì„¤ì •ê³¼ ë™ì¼í•´ì•¼ í•¨)
    NEW_TOKENS_PER_STEP = CONTEXT_WINDOW // 2  # í•œ ë²ˆì— ìƒì„±í•  ê¸¸ì´ (CONTEXT_WINDOWì˜ ì ˆë°˜ ì¶”ì²œ)

    log.info(f">> ëª©í‘œ ê¸¸ì´: {TARGET_LENGTH} í† í° (ìŠ¬ë¼ì´ë”© ë°©ì‹)")
    pbar = tqdm(total=TARGET_LENGTH, desc="ì‘ê³¡ ì¤‘")
    
    # Initial sequence based on composer or start token
    if composer_token_id is not None:
        current_ids = [composer_token_id, tokenizer.bar_token_id]
    else:
        current_ids = [tokenizer.start_token_id, tokenizer.bar_token_id]

    full_generated_sequence = list(current_ids)
    pbar.update(len(full_generated_sequence))

    while len(full_generated_sequence) < TARGET_LENGTH:
        # 1) ì…ë ¥ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (ê°€ì¥ ìµœê·¼ í† í°ë“¤ë§Œ ì˜ë¼ì„œ ê°€ì ¸ì˜´)
        max_context = CONTEXT_WINDOW - NEW_TOKENS_PER_STEP
        input_ids = full_generated_sequence[-max_context:]

        # 2) ìƒì„±
        with torch.no_grad():
            gen_len = len(input_ids) + NEW_TOKENS_PER_STEP
            output = model.generate(
                input_ids=torch.tensor([input_ids]).to(device),
                max_length=gen_len,
                do_sample=True,
                temperature=1.0,
                top_p=0.9,
                top_k=40,
                repetition_penalty=1.15,
                no_repeat_ngram_size=32,
                pad_token_id=tokenizer.pad_token_id # Use tokenizer's pad_token_id
            )

        # 3) ìƒˆë¡œìš´ í† í°ë§Œ ì¶”ì¶œí•´ì„œ ì „ì²´ ì‹œí€€ìŠ¤ì— ì¶”ê°€
        new_tokens = output[0, len(input_ids):].tolist()
        full_generated_sequence.extend(new_tokens)
        pbar.update(len(new_tokens))

    pbar.close()

    # 5. ì €ì¥
    log.info("\n>> ë³€í™˜ ë° ì €ì¥ ì¤‘...")
    
    # Filter out tokens that are out of tokenizer's vocabulary range if necessary
    # This might be important if composer_token_id or other special tokens are handled outside the tokenizer's core vocab.
    final_midi_ids = [t for t in full_generated_sequence if t < tokenizer.vocab_size]

    output_dir = Path("generated_output") / cfg.project_name
    os.makedirs(output_dir, exist_ok=True)
    save_path = output_dir / f"output_{target_composer.replace(' ', '_')}_long.mid"
    
    # Use the abstracted tokenizer's decode method
    tokenizer.decode(final_midi_ids, Path(save_path))
    
    log.info(f"\n=== ğŸ¹ ê¸´ ê³¡ ì‘ê³¡ ì™„ë£Œ! ì €ì¥ë¨: {save_path} ===")

if __name__ == "__main__":
    main()